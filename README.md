# AFlow + GRPO æ™ºèƒ½ä½“å·¥ä½œæµè®­ç»ƒæ¡†æ¶

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº† **AFlow + ROLL GRPO** è®­ç»ƒæ¡†æ¶ï¼Œç”¨äºè®­ç»ƒå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæ™ºèƒ½ä½“å·¥ä½œæµï¼ˆAgent Workflowï¼‰ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸš€ **GRPO è®­ç»ƒ**: Group Relative Policy Optimizationï¼Œæ— éœ€ Critic æ¨¡å‹
- ğŸ”§ **WA-GRPO**: Workflow-Aware ä¼˜åŠ¿è®¡ç®—ï¼Œè€ƒè™‘å¤šæ ·æ€§å’Œæ”¹è¿›å¹…åº¦
- ğŸ¯ **LoRA å¾®è°ƒ**: ä½èµ„æºé«˜æ•ˆè®­ç»ƒï¼Œä»…éœ€ 40M å¯è®­ç»ƒå‚æ•°
- ğŸ¤– **LLM Judge**: ä½¿ç”¨ OpenAI gpt-4o-mini ä½œä¸ºè¯„ä¼°å™¨
- ğŸ“Š **å¤šé¢†åŸŸæ”¯æŒ**: æ•°å­¦ã€ç¼–ç¨‹ã€é—®ç­”ä¸‰å¤§é¢†åŸŸ

### æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è®­ç»ƒæµç¨‹                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å…¥é—®é¢˜ â†’ æ¨¡å‹ç”Ÿæˆå·¥ä½œæµ â†’ AFlowæ‰§è¡Œ â†’ LLMè¯„ä¼° â†’ GRPOæ›´æ–°  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Qwen2.5    â”‚    â”‚    AFlow     â”‚    â”‚   OpenAI     â”‚
â”‚  7B-Instruct â”‚ â†’  â”‚   Executor   â”‚ â†’  â”‚  gpt-4o-mini â”‚
â”‚  (LoRAå¾®è°ƒ)  â”‚    â”‚  (ç®—å­æ‰§è¡Œ)   â”‚    â”‚  (LLM Judge) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ–¥ï¸ ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

| ç»„ä»¶ | æœ€ä½é…ç½® | æ¨èé…ç½® |
|------|---------|----------|
| GPU | V100 16GB | A100 40GB |
| å†…å­˜ | 32GB | 64GB |
| å­˜å‚¨ | 50GB | 100GB |

### è½¯ä»¶è¦æ±‚

| è½¯ä»¶ | ç‰ˆæœ¬è¦æ±‚ | æµ‹è¯•ç‰ˆæœ¬ |
|------|----------|----------|
| Python | 3.10+ | 3.10.12 |
| CUDA | 12.0+ | 12.6 |
| PyTorch | 2.0+ | 2.9.0 |
| transformers | 4.40+ | 4.57.2 |
| peft | 0.10+ | 0.18.0 |
| openai | 1.0+ | 2.8.1 |

---

## ğŸš€ Google Colab å¿«é€Ÿå¼€å§‹

### æ–¹å¼ä¸€ï¼šä¸€é”®å¯åŠ¨ (æ¨è)

å¤åˆ¶ä»¥ä¸‹ä»£ç åˆ° Colab å•å…ƒæ ¼å¹¶è¿è¡Œï¼š

```python
#@title ğŸš€ AFlow + GRPO ä¸€é”®å¯åŠ¨
#@markdown ### é…ç½®å‚æ•°
OPENAI_API_KEY = "sk-your-api-key-here"  #@param {type:"string"}
USE_WANDB = False  #@param {type:"boolean"}
WANDB_API_KEY = ""  #@param {type:"string"}

import os

# ======== Step 1: æ£€æŸ¥ GPU ========
print("ğŸ” æ£€æŸ¥ GPU...")
!nvidia-smi --query-gpu=name,memory.total --format=csv

# ======== Step 2: å…‹éš†ä»“åº“ ========
print("\nğŸ“¥ å…‹éš†ä»“åº“...")
!git clone https://github.com/beita6969/colab.git 2>/dev/null || (cd colab && git pull)
%cd colab

# ======== Step 3: å®‰è£…ä¾èµ– ========
print("\nğŸ“¦ å®‰è£…ä¾èµ– (çº¦2-3åˆ†é’Ÿ)...")
!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
!pip install -q transformers>=4.40.0 accelerate>=0.27.0 peft>=0.10.0
!pip install -q bitsandbytes>=0.42.0 scipy safetensors
!pip install -q openai httpx pyyaml tqdm wandb
!pip install -q datasets sentencepiece tiktoken huggingface-hub

# ======== Step 4: é…ç½®ç¯å¢ƒå˜é‡ ========
print("\nâš™ï¸ é…ç½®ç¯å¢ƒ...")
os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY
os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'
os.environ['PYTHONUNBUFFERED'] = '1'

if USE_WANDB and WANDB_API_KEY:
    os.environ['WANDB_API_KEY'] = WANDB_API_KEY
    print("âœ… WandB å·²é…ç½®")

# ======== Step 5: éªŒè¯ç¯å¢ƒ ========
print("\nğŸ”¬ éªŒè¯ç¯å¢ƒ...")
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# ======== Step 6: å¯åŠ¨è®­ç»ƒ ========
print("\nğŸš€ å¯åŠ¨è®­ç»ƒ...")
print("="*50)
!python3 train.py --config config/training.yaml
```

### æ–¹å¼äºŒï¼šåˆ†æ­¥æ‰§è¡Œ

#### Step 1: è®¾ç½® Colab è¿è¡Œæ—¶

1. ç‚¹å‡»èœå• `è¿è¡Œæ—¶` â†’ `æ›´æ”¹è¿è¡Œæ—¶ç±»å‹`
2. ç¡¬ä»¶åŠ é€Ÿå™¨é€‰æ‹© `GPU`
3. GPU ç±»å‹é€‰æ‹© `A100`ï¼ˆå¦‚æœ‰ï¼‰æˆ– `V100` / `T4`

#### Step 2: æ£€æŸ¥ GPU

```python
!nvidia-smi

# é¢„æœŸè¾“å‡ºç¤ºä¾‹:
# NVIDIA A100-SXM4-40GB, 40960MiB
```

#### Step 3: å…‹éš†ä»“åº“

```bash
!git clone https://github.com/beita6969/colab.git
%cd colab
```

#### Step 4: å®‰è£…ä¾èµ–

```bash
# PyTorch (CUDA 12.6)
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# æ ¸å¿ƒä¾èµ–
!pip install -r requirements.txt
```

#### Step 5: é…ç½® API Key

**æ–¹æ³• A: ç›´æ¥è®¾ç½®**
```python
import os
os.environ['OPENAI_API_KEY'] = 'sk-your-openai-api-key'
os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'
```

**æ–¹æ³• B: ä½¿ç”¨ Colab Secrets (æ¨èï¼Œæ›´å®‰å…¨)**
```python
from google.colab import userdata
import os
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')
```

#### Step 6: å¯åŠ¨è®­ç»ƒ

```bash
!python3 train.py --config config/training.yaml
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ train.py                    # ğŸš€ è®­ç»ƒå…¥å£
â”œâ”€â”€ requirements.txt            # ğŸ“¦ Python ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ setup_env.sh               # âš™ï¸ ç¯å¢ƒé…ç½®è„šæœ¬ (bash)
â”œâ”€â”€ COLAB_SETUP.md             # ğŸ“– Colab ç¯å¢ƒè¯´æ˜
â”‚
â”œâ”€â”€ config/                     # âš™ï¸ é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ training.yaml          # ä¸»è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ aflow_llm.yaml         # LLM API é…ç½®
â”‚   â”œâ”€â”€ operator.json          # AFlow ç®—å­æè¿°
â”‚   â”œâ”€â”€ judge_prompts.yaml     # LLM Judge æç¤ºè¯
â”‚   â””â”€â”€ datasets.yaml          # æ•°æ®é›†é…ç½®
â”‚
â”œâ”€â”€ src/                        # ğŸ”§ æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ grpo_trainer.py        # GRPO è®­ç»ƒå™¨ä¸»é€»è¾‘
â”‚   â”œâ”€â”€ aflow_executor.py      # AFlow å·¥ä½œæµæ‰§è¡Œå™¨
â”‚   â”œâ”€â”€ reward_computer.py     # å¥–åŠ±è®¡ç®—æ¨¡å—
â”‚   â”œâ”€â”€ wa_grpo.py             # WA-GRPO ä¼˜åŠ¿ä¼°è®¡
â”‚   â”œâ”€â”€ answer_extractor.py    # ç­”æ¡ˆæå–å™¨
â”‚   â”œâ”€â”€ data_manager.py        # æ•°æ®ç®¡ç†
â”‚   â”œâ”€â”€ gpu_manager.py         # GPU èµ„æºç®¡ç†
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ scripts/                    # ğŸ“œ å·¥å…·è„šæœ¬
â”‚   â”œâ”€â”€ async_llm.py           # å¼‚æ­¥ LLM å®¢æˆ·ç«¯ (OpenAI)
â”‚   â”œâ”€â”€ operators.py           # AFlow å·¥ä½œæµç®—å­
â”‚   â”œâ”€â”€ evaluator.py           # è¯„ä¼°å™¨ (DatasetType æšä¸¾)
â”‚   â”œâ”€â”€ download_datasets.py   # ä¸‹è½½æ•°æ®é›†
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ data/                       # ğŸ“Š æ•°æ®ç›®å½•
    â”œâ”€â”€ ready_to_train/        # é¢„å¤„ç†åçš„è®­ç»ƒæ•°æ®
    â”‚   â”œâ”€â”€ train_10k_final.jsonl
    â”‚   â””â”€â”€ test_500_preprocessed.jsonl
    â”œâ”€â”€ gsm8k/                 # GSM8K æ•°å­¦æ•°æ®
    â”œâ”€â”€ humaneval/             # HumanEval ä»£ç æ•°æ®
    â””â”€â”€ hotpotqa/              # HotpotQA é—®ç­”æ•°æ®
```

---

## âš™ï¸ é…ç½®è¯¦è§£

### è®­ç»ƒé…ç½® (`config/training.yaml`)

```yaml
# ========== GRPO ç®—æ³• ==========
num_return_sequences_in_group: 2   # Kå€¼: æ¯ä¸ªé—®é¢˜ç”ŸæˆKä¸ªå·¥ä½œæµ
rollout_batch_size: 5              # Bå€¼: æ¯æ‰¹å¤„ç†Bä¸ªé—®é¢˜
# å®é™…æ¯æ­¥æ ·æœ¬æ•° = K Ã— B = 2 Ã— 5 = 10

# ========== å­¦ä¹ å‚æ•° ==========
learning_rate: 2.0e-5              # å­¦ä¹ ç‡
max_steps: 500                     # æœ€å¤§è®­ç»ƒæ­¥æ•°
warmup_steps: 100                  # é¢„çƒ­æ­¥æ•° (20%)
kl_loss_coef: 0.005                # KL æ•£åº¦æƒ©ç½šç³»æ•°
clip_range: 0.20                   # PPO è£å‰ªèŒƒå›´

# ========== LoRA é…ç½® ==========
lora_rank: 64                      # LoRA çŸ©é˜µç§©
lora_alpha: 64                     # LoRA ç¼©æ”¾å› å­
lora_target_modules: "q_proj,k_proj,v_proj,o_proj"  # ç›®æ ‡æ¨¡å—
lora_dropout: 0.05                 # Dropout ç‡

# ========== WA-GRPO é…ç½® ==========
wa_grpo:
  diversity_weight: 0.35           # å·¥ä½œæµå¤šæ ·æ€§æƒé‡
  revise_gain_weight: 0.25         # ä¿®è®¢æ”¹è¿›æƒé‡
  exec_success_weight: 0.20        # æ‰§è¡ŒæˆåŠŸç‡æƒé‡
  efficiency_weight: 0.10          # æ•ˆç‡æƒé‡
  op_variety_weight: 0.10          # ç®—å­å¤šæ ·æ€§æƒé‡

# ========== æ¸©åº¦è°ƒåº¦ ==========
temperature_schedule:
  enabled: true                    # å¯ç”¨åŠ¨æ€æ¸©åº¦
  initial: 0.5                     # åˆå§‹æ¸©åº¦ (é«˜æ¢ç´¢)
  final: 0.15                      # æœ€ç»ˆæ¸©åº¦ (ä½æ¢ç´¢)
  warmup_steps: 150                # è¡°å‡æ­¥æ•°
```

### æ˜¾å­˜é…ç½®å»ºè®®

| GPU | æ˜¾å­˜ | K | B | grad_accum | è¯´æ˜ |
|-----|------|---|---|------------|------|
| T4 | 16GB | 2 | 2 | 8 | æœ€å°é…ç½® |
| V100 | 16GB | 2 | 3 | 6 | æ¨è |
| A100 | 40GB | 2 | 5 | 4 | **é»˜è®¤é…ç½®** |
| A100 | 80GB | 4 | 8 | 2 | é«˜åå |

---

## ğŸ”§ å¸¸è§é—®é¢˜ (FAQ)

### Q1: CUDA åº“æ‰¾ä¸åˆ°

**é”™è¯¯ä¿¡æ¯:**
```
OSError: libcudart.so.12: cannot open shared object file
```

**è§£å†³æ–¹æ¡ˆ:**
```python
import os
os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia:/usr/local/cuda/lib64'
```

æˆ–åœ¨ç»ˆç«¯è¿è¡Œ:
```bash
export LD_LIBRARY_PATH=/usr/lib64-nvidia:/usr/local/cuda/lib64:$LD_LIBRARY_PATH
```

---

### Q2: OpenAI API è®¤è¯å¤±è´¥

**é”™è¯¯ä¿¡æ¯:**
```
openai.AuthenticationError: Invalid API key provided
```

**è§£å†³æ–¹æ¡ˆ:**
1. æ£€æŸ¥ API Key æ˜¯å¦æ­£ç¡®
2. ç¡®ä¿è®¾ç½®äº†ç¯å¢ƒå˜é‡:
```python
import os
os.environ['OPENAI_API_KEY'] = 'sk-proj-xxx'  # æ›¿æ¢ä¸ºä½ çš„ key
```

---

### Q3: æ˜¾å­˜ä¸è¶³ (OOM)

**é”™è¯¯ä¿¡æ¯:**
```
torch.cuda.OutOfMemoryError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ:** ä¿®æ”¹ `config/training.yaml`:
```yaml
rollout_batch_size: 2              # å‡å°æ‰¹å¤§å°
gradient_accumulation_steps: 8     # å¢åŠ ç´¯ç§¯æ­¥æ•°
gradient_checkpointing: true       # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
```

---

### Q4: æ¨¡å‹ä¸‹è½½æ…¢

**è§£å†³æ–¹æ¡ˆ:** ä½¿ç”¨ HuggingFace é•œåƒ:
```python
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

---

### Q5: WandB è¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ:** ç¦ç”¨ WandB:
```yaml
# config/training.yaml
wandb:
  enabled: false
```

---

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### ä½¿ç”¨ WandB (æ¨è)

1. æ³¨å†Œè´¦å·: https://wandb.ai
2. è·å– API Key: https://wandb.ai/settings
3. é…ç½®:
```yaml
# config/training.yaml
wandb:
  enabled: true
  project: "agent-prompt"
  api_key: "your-wandb-api-key"
```

### æŸ¥çœ‹æœ¬åœ°æ—¥å¿—

```bash
# å®æ—¶æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f training.log

# ç­›é€‰å…³é”®æŒ‡æ ‡
grep -E "Step|reward|loss|accuracy" training.log | tail -50
```

---

## ğŸ”„ æ¢å¤è®­ç»ƒ

å¦‚æœ Colab æ–­å¼€è¿æ¥æˆ–è®­ç»ƒä¸­æ–­:

```python
# 1. æŸ¥çœ‹å·²ä¿å­˜çš„ checkpoints
!ls -la checkpoints/

# 2. ä»æœ€æ–° checkpoint æ¢å¤
!python3 train.py --config config/training.yaml --resume checkpoints/step_100
```

---

## ğŸ“š AFlow ç®—å­è¯´æ˜

| ç®—å­ | åŠŸèƒ½ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| `Custom` | è‡ªå®šä¹‰æŒ‡ä»¤æ‰§è¡Œ | é€šç”¨é—®é¢˜ |
| `AnswerGenerate` | æ­¥éª¤æ¨ç† | æ•°å­¦é¢˜ |
| `Programmer` | ä»£ç ç”Ÿæˆæ‰§è¡Œ | ç¼–ç¨‹é¢˜ |
| `Test` | ä»£ç æµ‹è¯• | éªŒè¯ä»£ç  |
| `Review` | è§£ç­”å®¡æŸ¥ | è´¨é‡æ£€æŸ¥ |
| `Revise` | è§£ç­”ä¿®è®¢ | æ”¹è¿›ç­”æ¡ˆ |
| `ScEnsemble` | è‡ªæ´½é›†æˆ | å¤šç­”æ¡ˆæŠ•ç¥¨ |

---

## ğŸ“ æ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ® JSONL æ ¼å¼:

```json
{"question": "What is 2 + 3?", "answer": "5", "source": "gsm8k"}
{"question": "def add(a, b): ...", "answer": "return a + b", "source": "humaneval"}
{"question": "Who wrote Romeo and Juliet?", "answer": "Shakespeare", "source": "hotpotqa"}
```

---

## ğŸ™ è‡´è°¢

- [AFlow](https://github.com/geekan/MetaGPT) - å·¥ä½œæµæ¡†æ¶
- [GRPO](https://arxiv.org/abs/2402.03300) - è®­ç»ƒç®—æ³•è®ºæ–‡
- [Qwen2.5](https://github.com/QwenLM/Qwen2.5) - åŸºç¡€æ¨¡å‹
- [OpenAI](https://openai.com) - LLM Judge API
- [PEFT](https://github.com/huggingface/peft) - LoRA å®ç°

---

## ğŸ“„ License

MIT License - è¯¦è§ [LICENSE](LICENSE)
