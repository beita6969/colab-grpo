#!/usr/bin/env python3
"""
vLLMå·¥ä½œæµç”Ÿæˆå™¨ - ä½¿ç”¨vLLM APIè¿›è¡Œå¹¶å‘æ¨ç†ï¼ˆFallback: ä½¿ç”¨transformersï¼‰
"""
import asyncio
import torch
from openai import AsyncOpenAI
from typing import Dict, List, Optional, Tuple
import json
import ast
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer

class VLLMWorkflowGenerator:
    """ä½¿ç”¨vLLM APIç”Ÿæˆä¼˜åŒ–çš„å·¥ä½œæµï¼ˆæ”¯æŒå¹¶å‘ï¼‰

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. vLLM APIæ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šé€šè¿‡AsyncOpenAIå®¢æˆ·ç«¯è°ƒç”¨vLLMæœåŠ¡
    2. Transformersæ¨¡å¼ï¼ˆFallbackï¼‰ï¼šç›´æ¥ä½¿ç”¨transformersåº“
    """

    def __init__(
        self,
        base_url: str = "http://localhost:8003/v1",
        api_key: str = "EMPTY",
        model_name: str = "/home/yijia/verl-agent/models/qwen/Qwen2___5-7B-Instruct",
        max_concurrent: int = 6,
        operator_descriptions_path: Optional[str] = None,
        config: Optional[Dict] = None,
        use_vllm_api: bool = False,  # é»˜è®¤ä½¿ç”¨transformersæ¨¡å¼
        device: str = "cuda:0"
    ):
        """
        Args:
            base_url: vLLMæœåŠ¡å™¨åœ°å€
            api_key: APIå¯†é’¥ï¼ˆvLLMä¸éœ€è¦çœŸå®å¯†é’¥ï¼‰
            model_name: æ¨¡å‹åç§°/è·¯å¾„
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
            operator_descriptions_path: AFlowç®—å­æè¿°æ–‡ä»¶è·¯å¾„
            config: é¢å¤–é…ç½®
            use_vllm_api: æ˜¯å¦ä½¿ç”¨vLLM APIï¼ˆFalseåˆ™ä½¿ç”¨transformersï¼‰
            device: è®¾å¤‡ï¼ˆtransformersæ¨¡å¼ï¼‰
        """
        self.model_name = model_name
        self.max_concurrent = max_concurrent
        self.config = config or {}
        self.use_vllm_api = use_vllm_api
        self.device = device

        # åŠ è½½ç®—å­æè¿°
        self.operator_descriptions = self._load_operator_descriptions(operator_descriptions_path)

        if use_vllm_api:
            # vLLM APIæ¨¡å¼
            self.client = AsyncOpenAI(
                base_url=base_url,
                api_key=api_key,
                timeout=300.0,  # 5åˆ†é’Ÿè¶…æ—¶
                max_retries=2
            )
            self.semaphore = asyncio.Semaphore(max_concurrent)
            print(f"âœ… åˆå§‹åŒ–vLLMå·¥ä½œæµç”Ÿæˆå™¨ï¼ˆAPIæ¨¡å¼ï¼‰")
            print(f"  æœåŠ¡å™¨: {base_url}")
            print(f"  æœ€å¤§å¹¶å‘: {max_concurrent}")
        else:
            # Transformersæ¨¡å¼ï¼ˆç›´æ¥ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹ï¼‰
            self.model = None  # å°†ç”±å¤–éƒ¨è®¾ç½®ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
            self.tokenizer = None
            # âš ï¸ å…³é”®ä¿®å¤ï¼šä½¿ç”¨é”ä¿æŠ¤GPUè®¿é—®ï¼ˆåŒä¸€æ—¶é—´åªå…è®¸ä¸€ä¸ªæ¨ç†ï¼‰
            self._generation_lock = asyncio.Lock()
            print(f"âœ… åˆå§‹åŒ–workflowç”Ÿæˆå™¨ï¼ˆTransformersæ¨¡å¼ï¼‰")
            print(f"  æ¨¡å‹: {model_name}")
            print(f"  è®¾å¤‡: {device}")
            print(f"  âš ï¸  GPUæ¨ç†å°†ä¸²è¡Œæ‰§è¡Œï¼ˆé¿å…CUDAå†²çªï¼‰")

    def _load_operator_descriptions(self, descriptions_path: Optional[str]) -> Dict:
        """åŠ è½½AFlowç®—å­æè¿°"""
        if descriptions_path and Path(descriptions_path).exists():
            with open(descriptions_path, 'r') as f:
                return json.load(f)

        # é»˜è®¤ç®—å­æè¿° - AFlowæ ‡å‡†10ä¸ªç®—å­
        return {
            "Custom": {
                "description": "Generates anything based on customized input and instruction.",
                "interface": "custom(input: str, instruction: str) -> dict with key 'response'"
            },
            "AnswerGenerate": {
                "description": "Generates step-by-step reasoning with thought process and final answer.",
                "interface": "answer_generate(input: str) -> dict with keys 'thought' and 'answer'"
            },
            "CustomCodeGenerate": {
                "description": "Generates code based on customized input and instruction.",
                "interface": "custom_code_generate(problem: str, entry_point: str, instruction: str) -> dict with key 'code'"
            },
            "Programmer": {
                "description": "Automatically writes and executes Python code, returns execution result.",
                "interface": "programmer(problem: str, analysis: str = 'None') -> dict with keys 'code' and 'output'"
            },
            "Test": {
                "description": "Tests code with test cases, reflects on errors and revises.",
                "interface": "test(problem: str, solution: str, entry_point: str, test_loop: int = 3) -> dict with keys 'result' and 'solution'"
            },
            "Format": {
                "description": "Extracts concise answer from verbose solution.",
                "interface": "format(problem: str, solution: str) -> dict with key 'solution'"
            },
            "Review": {
                "description": "Reviews solution correctness using critical thinking.",
                "interface": "review(problem: str, solution: str) -> dict with keys 'review_result' (bool) and 'feedback'"
            },
            "Revise": {
                "description": "Revises solution based on feedback.",
                "interface": "revise(problem: str, solution: str, feedback: str) -> dict with key 'solution'"
            },
            "ScEnsemble": {
                "description": "Uses self-consistency to select the most frequent solution.",
                "interface": "sc_ensemble(solutions: List[str], problem: str) -> dict with key 'response'"
            },
            "MdEnsemble": {
                "description": "Majority voting ensemble - shuffles and votes multiple times (more robust than ScEnsemble).",
                "interface": "md_ensemble(solutions: List[str], problem: str) -> dict with key 'solution'"
            }
        }

    def _build_generation_prompt(self, problem: str, problem_type: str) -> str:
        """æ„å»ºç”Ÿæˆæç¤ºè¯ - å¼€æ”¾å¼DSLç»„åˆæ ¼å¼

        è®©æ¨¡å‹è‡ªç”±ç»„åˆoperatorsï¼Œè€Œä¸æ˜¯ä»é¢„è®¾é€‰é¡¹ä¸­é€‰æ‹©
        """
        prompt = f"""Design a workflow to solve this problem. Output a single-line DSL expression.

Available Operators:
- Custom: General reasoning, text generation. (input, instruction) -> response
- Programmer: Auto-execute Python code for calculations. (problem, analysis) -> code, output
- ScEnsemble: Vote on multiple solutions. (solutions[], problem) -> response
- Review: Check if solution is correct. (problem, solution) -> review_result, feedback
- Revise: Fix solution based on feedback. (problem, solution, feedback) -> solution

DSL Syntax:
- Single operator: Custom
- Chain (sequential): Custom -> Programmer -> Custom
- Parallel then merge: [Custom, Custom, Custom] -> ScEnsemble
- Conditional: Review ? Revise : done
- Loop: (Custom -> Review -> Revise) * 3

Examples:
- Simple QA: Custom
- Math calculation: Programmer
- Complex math: Programmer -> Custom
- Need multiple attempts: [Custom, Custom, Custom] -> ScEnsemble
- Self-correction: Custom -> Review -> Revise
- Code generation: Programmer -> Review ? Revise : done

Problem ({problem_type}): {problem}

DSL:"""
        return prompt

    async def generate_workflow(
        self,
        problem: str,
        problem_type: str = "math",
        temperature: float = 0.7,
        max_new_tokens: int = 2048,
        custom_prompt: Optional[str] = None
    ) -> Dict:
        """
        ç”Ÿæˆå•ä¸ªå·¥ä½œæµï¼ˆå¼‚æ­¥ï¼‰

        Returns:
            {
                "workflow_code": "Pythonä»£ç ",
                "valid": bool,
                "error": Optional[str],
                "metadata": {...}
            }
        """
        if self.use_vllm_api:
            return await self._generate_with_vllm_api(
                problem, problem_type, temperature, max_new_tokens, custom_prompt
            )
        else:
            return await self._generate_with_transformers(
                problem, problem_type, temperature, max_new_tokens, custom_prompt
            )

    async def _generate_with_vllm_api(
        self,
        problem: str,
        problem_type: str,
        temperature: float,
        max_tokens: int,
        custom_prompt: Optional[str]
    ) -> Dict:
        """ä½¿ç”¨vLLM APIç”Ÿæˆ"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            try:
                # æ„å»ºæç¤ºè¯
                prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

                # è°ƒç”¨vLLM API
                response = await self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": "You are a workflow generation expert."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=self.config.get('top_p', 0.95),
                )

                # æå–ç”Ÿæˆçš„ä»£ç 
                generated_text = response.choices[0].message.content
                workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

                return {
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "tokens": response.usage.total_tokens if response.usage else 0,
                        "model": self.model_name
                    }
                }

            except Exception as e:
                return {
                    "workflow_code": "",
                    "valid": False,
                    "error": str(e),
                    "metadata": {}
                }

    async def _generate_with_transformers(
        self,
        problem: str,
        problem_type: str,
        temperature: float,
        max_new_tokens: int,
        custom_prompt: Optional[str]
    ) -> Dict:
        """ä½¿ç”¨transformersç”Ÿæˆï¼ˆä½¿ç”¨é”ä¿æŠ¤GPUè®¿é—®ï¼‰"""
        # âš ï¸ å…³é”®ï¼šä½¿ç”¨é”ç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªæ¨ç†åœ¨æ‰§è¡Œ
        async with self._generation_lock:
            loop = asyncio.get_event_loop()

            def _sync_generate():
                """åŒæ­¥ç”Ÿæˆå‡½æ•°ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
                # æ„å»ºæç¤ºè¯
                prompt = custom_prompt or self._build_generation_prompt(problem, problem_type)

                # Tokenize
                inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

                # ç”Ÿæˆ
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        temperature=temperature,
                        top_p=self.config.get('top_p', 0.95),
                        top_k=self.config.get('top_k', 50),
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )

                # è§£ç 
                generated_text = self.tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )

                return generated_text

            try:
                # åœ¨é»˜è®¤executorä¸­è¿è¡Œï¼ˆCPUå¯†é›†å‹æ“ä½œï¼‰
                generated_text = await loop.run_in_executor(None, _sync_generate)

                # è§£æè¾“å‡º
                workflow_code, is_valid, error = self._parse_workflow_code(generated_text, problem_type)

                return {
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problem,
                        "problem_type": problem_type,
                        "temperature": temperature
                    }
                }
            except Exception as e:
                return {
                    "workflow_code": "",
                    "valid": False,
                    "error": str(e),
                    "metadata": {}
                }

    def _parse_workflow_code(self, generated_text: str, problem_type: str) -> Tuple[str, bool, Optional[str]]:
        """è§£æç”Ÿæˆçš„æ–‡æœ¬ï¼Œæå–å¹¶éªŒè¯å·¥ä½œæµä»£ç 

        æ”¯æŒå¼€æ”¾å¼DSLæ ¼å¼ï¼š
        - å•ä¸€ç®—å­: Custom
        - é“¾å¼: Custom -> Programmer -> Custom
        - å¹¶è¡Œ: [Custom, Custom, Custom] -> ScEnsemble
        - æ¡ä»¶: Review ? Revise : done
        """
        import re

        # ğŸ”§ é¦–å…ˆå°è¯•ç›´æ¥è§£æDSLï¼ˆæ¨¡å‹è¾“å‡ºçš„ç¬¬ä¸€è¡Œï¼‰
        text_clean = generated_text.strip()
        first_line = text_clean.split('\n')[0].strip()

        # æ£€æŸ¥æ˜¯å¦åŒ…å«operatoråç§°
        valid_ops = ['Custom', 'Programmer', 'ScEnsemble', 'Review', 'Revise', 'AnswerGenerate', 'CustomCodeGenerate', 'Test', 'Format', 'MdEnsemble']
        if any(op in first_line for op in valid_ops):
            # æ¸…ç†DSLï¼ˆç§»é™¤å¯èƒ½çš„å‰ç¼€å¦‚"DSL: "ï¼‰
            dsl_text = re.sub(r'^[^A-Za-z\[]*', '', first_line)
            dsl_text = re.sub(r'[^A-Za-z\]>\-,\s\?\*\(\):]*$', '', dsl_text).strip()
            if dsl_text:
                print(f"  ğŸ“ æ£€æµ‹åˆ°å¼€æ”¾å¼DSL: {dsl_text}")
                generator = WorkflowCodeGenerator(problem_type)
                code, is_valid, error = generator.generate(dsl_text)
                if is_valid:
                    print(f"  âœ… DSLæˆåŠŸè½¬æ¢ä¸ºä»£ç ")
                    return code, True, None
                else:
                    print(f"  âš ï¸ DSLè§£æå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ–¹æ³•: {error}")

        # ğŸ”§ å°è¯•æå–DSLæ ¼å¼ <workflow>...</workflow>
        workflow_match = re.search(r'<workflow>\s*([\s\S]*?)\s*(?:</workflow>|$)', generated_text)
        if workflow_match:
            dsl_text = workflow_match.group(1).strip()
            print(f"  ğŸ“ æ£€æµ‹åˆ°XML DSLæ ¼å¼: {dsl_text}")
            generator = WorkflowCodeGenerator(problem_type)
            code, is_valid, error = generator.generate(dsl_text)
            if is_valid:
                print(f"  âœ… DSLæˆåŠŸè½¬æ¢ä¸ºä»£ç ")
                return code, True, None
            else:
                print(f"  âš ï¸ DSLè§£æå¤±è´¥: {error}")

        # ğŸ”§ å°è¯•é€è¡Œå¯»æ‰¾æœ‰æ•ˆDSL
        for line in text_clean.split('\n'):
            line = line.strip()
            if line and any(op in line for op in valid_ops):
                line = re.sub(r'^[^A-Za-z\[]*', '', line)
                line = re.sub(r'[^A-Za-z\]>\-,\s\?\*\(\):]*$', '', line)
                if line and '->' in line or '[' in line or line in valid_ops:
                    print(f"  ğŸ“ å°è¯•è¡Œçº§DSLè§£æ: {line}")
                    generator = WorkflowCodeGenerator(problem_type)
                    code, is_valid, error = generator.generate(line)
                    if is_valid:
                        print(f"  âœ… è¡Œçº§DSLæˆåŠŸ")
                        return code, True, None

        # ğŸ”§ å°è¯•æå–æ—§XMLæ ¼å¼ <graph>...</graph>
        graph_code, prompt_code = self._extract_xml_workflow(generated_text)
        if graph_code:
            print(f"  ğŸ“ æ£€æµ‹åˆ°XMLæ ¼å¼å·¥ä½œæµ")
            code = graph_code.strip()
            if prompt_code:
                prompt_custom_code = prompt_code.strip()
            else:
                prompt_custom_code = self._get_default_prompt_custom(problem_type)
        else:
            # å›é€€åˆ°é»˜è®¤workflow
            print(f"  âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤workflow")
            return self._get_default_workflow(problem_type), False, "No valid format detected"

        if "TASK_PROMPT" not in code and prompt_custom_code:
            class_match = re.search(r'^class Workflow', code, re.MULTILINE)
            if class_match:
                code = prompt_custom_code + "\n\n" + code
            else:
                code = prompt_custom_code + "\n" + code

        code = self._validate_and_fix_workflow(code, problem_type)

        try:
            ast.parse(code)
            return code, True, None
        except SyntaxError as e:
            return self._get_default_workflow(problem_type), False, f"Syntax error: {str(e)}"

    def _extract_xml_workflow(self, text: str) -> Tuple[str, str]:
        """ä»XMLæ ¼å¼æå–graphå’Œpromptä»£ç 

        Returns:
            (graph_code, prompt_code) - å¦‚æœæœªæ‰¾åˆ°XMLæ ¼å¼åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        import re

        graph_code = ""
        prompt_code = ""

        # å°è¯•æå– <graph>...</graph>
        graph_match = re.search(r'<graph>\s*([\s\S]*?)\s*</graph>', text)
        if graph_match:
            graph_code = graph_match.group(1).strip()

        # å°è¯•æå– <prompt>...</prompt>
        prompt_match = re.search(r'<prompt>\s*([\s\S]*?)\s*</prompt>', text)
        if prompt_match:
            prompt_code = prompt_match.group(1).strip()

        return graph_code, prompt_code

    def _parse_legacy_format(self, generated_text: str, problem_type: str) -> Tuple[str, str]:
        """è§£ææ—§æ ¼å¼ï¼ˆPythonä»£ç å—æˆ–ç›´æ¥classå®šä¹‰ï¼‰"""
        import re

        # æå–ä»£ç å—
        code_start = generated_text.find("```python")
        if code_start == -1:
            code_start = generated_text.find("class Workflow:")
            if code_start == -1:
                return "", ""
            code = generated_text[code_start:]
        else:
            code_start += len("```python\n")
            code_end = generated_text.find("```", code_start)
            code = generated_text[code_start:code_end] if code_end != -1 else generated_text[code_start:]

        code = code.strip()

        # è§£æå¹¶æå–prompt_customéƒ¨åˆ†
        prompt_custom_start = code.find("# === PROMPT_CUSTOM START ===")
        prompt_custom_end = code.find("# === PROMPT_CUSTOM END ===")

        prompt_custom_code = ""
        if prompt_custom_start != -1 and prompt_custom_end != -1:
            end_line_end = code.find("\n", prompt_custom_end)
            if end_line_end == -1:
                end_line_end = len(code)
            prompt_custom_code = code[prompt_custom_start:end_line_end + 1]
            # ç§»é™¤åŸä½ç½®çš„prompt_custom
            code = code[:prompt_custom_start] + code[end_line_end + 1:]
        else:
            # å°è¯•æ£€æµ‹TASK_PROMPTå˜é‡å®šä¹‰
            task_prompt_match = re.search(
                r'^(TASK_PROMPT\s*=\s*(?:"""[\s\S]*?"""|\'\'\' [\s\S]*?\'\'\'))',
                code,
                re.MULTILINE
            )
            if task_prompt_match:
                prompt_custom_code = task_prompt_match.group(1)
            else:
                prompt_custom_code = self._get_default_prompt_custom(problem_type)

        return code.strip(), prompt_custom_code

    def _get_default_prompt_custom(self, problem_type: str) -> str:
        """è·å–é»˜è®¤çš„TASK_PROMPT"""
        if problem_type == "math":
            return '''TASK_PROMPT = """Solve this mathematical problem step by step.
Show your reasoning clearly and provide the final numerical answer.
Format: First explain your approach, then show calculations, finally state the answer."""'''
        elif problem_type == "code":
            return '''TASK_PROMPT = """Write a Python function to solve this problem.
Requirements:
1. The function should be efficient and handle edge cases
2. Include proper input validation
3. Return the correct type as specified"""'''
        else:
            return '''TASK_PROMPT = """Solve this problem carefully.
Provide a clear, structured answer with reasoning."""'''

    def _validate_and_fix_workflow(self, code: str, problem_type: str) -> str:
        """éªŒè¯å¹¶è‡ªåŠ¨ä¿®å¤workflowä¸­ç¼ºå¤±çš„operatoråˆå§‹åŒ–

        Args:
            code: ç”Ÿæˆçš„workflowä»£ç 
            problem_type: é—®é¢˜ç±»å‹

        Returns:
            ä¿®å¤åçš„ä»£ç 
        """
        import re

        # 1. æå–__init__ä¸­å·²åˆå§‹åŒ–çš„operators
        initialized_ops = set()
        init_section = re.search(r'def __init__\([^)]+\):[\s\S]+?(?=\n    async def|\n    def|$)', code)
        if init_section:
            init_code = init_section.group(0)
            # åŒ¹é… self.xxx = operator.XXX(self.llm)
            init_patterns = re.findall(r'self\.(\w+)\s*=\s*operator\.(\w+)\(', init_code)
            for attr_name, op_name in init_patterns:
                initialized_ops.add(attr_name)

        # 2. æå–__call__ä¸­ä½¿ç”¨çš„operators
        used_ops = set()
        call_section = re.search(r'async def __call__\([^)]+\):[\s\S]+', code)
        if call_section:
            call_code = call_section.group(0)
            # åŒ¹é… await self.xxx(...)
            used_patterns = re.findall(r'await self\.(\w+)\(', call_code)
            for op_name in used_patterns:
                used_ops.add(op_name)

        # 3. æ‰¾å‡ºç¼ºå¤±çš„operators
        missing_ops = used_ops - initialized_ops

        if missing_ops:
            print(f"\nâš ï¸  æ£€æµ‹åˆ°ç¼ºå¤±çš„operatoråˆå§‹åŒ–: {missing_ops}")
            print(f"   å·²åˆå§‹åŒ–: {initialized_ops}")
            print(f"   å·²ä½¿ç”¨: {used_ops}")

            # 4. è‡ªåŠ¨æ·»åŠ ç¼ºå¤±çš„åˆå§‹åŒ–ä»£ç 
            # æ‰¾åˆ° self.llm = create_llm_instance(...) çš„ä½ç½®
            llm_init_match = re.search(r'(\s+)(self\.llm = create_llm_instance\([^)]+\))', code)
            if llm_init_match:
                indent = llm_init_match.group(1)
                llm_init_line = llm_init_match.group(2)

                # æ„å»ºç¼ºå¤±çš„åˆå§‹åŒ–ä»£ç 
                missing_inits = []
                for op_name in sorted(missing_ops):
                    # æ¨æ–­operatorç±»åï¼ˆé¦–å­—æ¯å¤§å†™+é©¼å³°å‘½åï¼‰
                    # answer_generate -> AnswerGenerate
                    # review -> Review
                    op_class_name = ''.join(word.capitalize() for word in op_name.split('_'))

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„operatorï¼ˆAFlowæ ‡å‡†10ä¸ªç®—å­ï¼‰
                    valid_operators = [
                        'Custom', 'AnswerGenerate', 'CustomCodeGenerate',
                        'Programmer', 'Test', 'Format',
                        'Review', 'Revise', 'ScEnsemble', 'MdEnsemble'
                    ]
                    if op_class_name in valid_operators:
                        missing_inits.append(f"{indent}self.{op_name} = operator.{op_class_name}(self.llm)")

                if missing_inits:
                    # åœ¨ self.llm = ... ä¹‹åæ’å…¥
                    insert_code = '\n' + '\n'.join(missing_inits)
                    code = code.replace(llm_init_line, llm_init_line + insert_code)
                    print(f"âœ… è‡ªåŠ¨æ·»åŠ äº† {len(missing_inits)} ä¸ªç¼ºå¤±çš„operatoråˆå§‹åŒ–")

        return code

    def _get_default_workflow(self, problem_type: str = "math") -> str:
        """é»˜è®¤å·¥ä½œæµ - åŒ…å«TASK_PROMPT"""
        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©åˆé€‚çš„é»˜è®¤prompt
        if problem_type == "math":
            task_prompt = '''"""Solve this mathematical problem step by step.
Show your complete reasoning process:
1. Identify what the problem is asking
2. List known information and variables
3. Apply relevant formulas or methods
4. Perform calculations carefully
5. State the final numerical answer clearly

IMPORTANT: Always verify your answer before providing it."""'''
        elif problem_type == "code":
            task_prompt = '''"""Write a Python function to solve this problem.
Requirements:
1. Handle all edge cases properly
2. Use efficient algorithms
3. Include proper input validation
4. Return the correct type as specified
5. Add brief comments for complex logic"""'''
        else:
            task_prompt = '''"""Solve this problem carefully and provide a clear answer.
Show your reasoning step by step."""'''

        return f"""# === PROMPT_CUSTOM START ===
TASK_PROMPT = {task_prompt}
# === PROMPT_CUSTOM END ===

import workspace.{problem_type}.workflows.template.operator as operator
from scripts.async_llm import create_llm_instance
from scripts.evaluator import DatasetType

class Workflow:
    def __init__(self, name: str, llm_config, dataset: DatasetType):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str, entry_point: str = "solve"):
        # entry_point used for code problems with Test operator
        solution = await self.custom(input=problem, instruction=TASK_PROMPT)
        return solution['response'], self.llm.get_usage_summary()["total_cost"]
"""

    async def generate_workflows_batch(
        self,
        problems: List[str],
        problem_types: List[str],
        temperatures: List[float],
        custom_prompts: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        æ‰¹é‡å¹¶å‘ç”Ÿæˆå·¥ä½œæµï¼ˆä¼˜åŒ–ç‰ˆï¼šä½¿ç”¨GPU batchæ¨ç†ï¼‰

        Args:
            problems: é—®é¢˜åˆ—è¡¨
            problem_types: é—®é¢˜ç±»å‹åˆ—è¡¨
            temperatures: æ¸©åº¦åˆ—è¡¨
            custom_prompts: è‡ªå®šä¹‰æç¤ºè¯åˆ—è¡¨

        Returns:
            ç»“æœåˆ—è¡¨
        """
        if self.use_vllm_api:
            # vLLM APIæ¨¡å¼ï¼šå¹¶å‘è°ƒç”¨
            tasks = []
            for i in range(len(problems)):
                task = self.generate_workflow(
                    problem=problems[i],
                    problem_type=problem_types[i],
                    temperature=temperatures[i],
                    custom_prompt=custom_prompts[i] if custom_prompts else None
                )
                tasks.append(task)

            results = await asyncio.gather(*tasks, return_exceptions=True)

            processed_results = []
            for result in results:
                if isinstance(result, Exception):
                    processed_results.append({
                        "workflow_code": "",
                        "valid": False,
                        "error": str(result),
                        "metadata": {}
                    })
                else:
                    processed_results.append(result)

            return processed_results
        else:
            # Transformersæ¨¡å¼ï¼šä½¿ç”¨GPU batchæ¨ç†ï¼ˆå…³é”®ä¼˜åŒ–ï¼ï¼‰
            return await self._batch_generate_with_transformers(
                problems, problem_types, temperatures, custom_prompts
            )

    async def _batch_generate_with_transformers(
        self,
        problems: List[str],
        problem_types: List[str],
        temperatures: List[float],
        custom_prompts: Optional[List[str]]
    ) -> List[Dict]:
        """ä½¿ç”¨transformersæ‰¹é‡ç”Ÿæˆï¼ˆGPU batchæ¨ç†ï¼Œæ”¯æŒåˆ†æ‰¹ä»¥é™ä½æ˜¾å­˜ï¼‰"""
        loop = asyncio.get_event_loop()

        # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹ç”Ÿæˆï¼Œæ¯æ‰¹æœ€å¤š8ä¸ªåºåˆ—
        MAX_BATCH_SIZE = 8  # æ¯æ‰¹æœ€å¤š8ä¸ªï¼Œé™ä½æ˜¾å­˜å³°å€¼

        def _sync_batch_generate(batch_prompts, batch_temp):
            """åŒæ­¥æ‰¹é‡ç”Ÿæˆå‡½æ•°ï¼ˆå•æ‰¹ï¼‰"""
            # æ‰¹é‡tokenizeï¼ˆå…³é”®ï¼špaddingå¯¹é½ï¼‰
            inputs = self.tokenizer(
                batch_prompts,
                return_tensors="pt",
                padding=True,  # å¯¹é½åˆ°æœ€é•¿åºåˆ—
                truncation=True,
                max_length=3072
            ).to(self.device)

            # æ‰¹é‡ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=self.config.get('max_new_tokens', 2048),
                    temperature=batch_temp,
                    top_p=self.config.get('top_p', 0.95),
                    top_k=self.config.get('top_k', 50),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            # æ‰¹é‡è§£ç 
            generated_texts = self.tokenizer.batch_decode(
                outputs[:, inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )

            # ğŸ”§ æ˜¾å­˜ä¼˜åŒ–ï¼šåŠæ—¶æ¸…ç†
            del inputs, outputs
            torch.cuda.empty_cache()

            return generated_texts

        try:
            # æ„å»ºæ‰€æœ‰prompts
            all_prompts = []
            for i in range(len(problems)):
                if custom_prompts and custom_prompts[i]:
                    prompt = custom_prompts[i]
                else:
                    prompt = self._build_generation_prompt(problems[i], problem_types[i])
                all_prompts.append(prompt)

            # ğŸ”§ åˆ†æ‰¹å¤„ç†ä»¥é™ä½æ˜¾å­˜å³°å€¼
            all_generated_texts = []
            for batch_start in range(0, len(all_prompts), MAX_BATCH_SIZE):
                batch_end = min(batch_start + MAX_BATCH_SIZE, len(all_prompts))
                batch_prompts = all_prompts[batch_start:batch_end]
                batch_temp = temperatures[batch_start]  # å‡è®¾åŒæ‰¹temperatureç›¸åŒ

                print(f"  ğŸ”§ ç”Ÿæˆæ‰¹æ¬¡ {batch_start//MAX_BATCH_SIZE + 1}/{(len(all_prompts)-1)//MAX_BATCH_SIZE + 1} ({len(batch_prompts)}ä¸ªåºåˆ—)")

                # åœ¨çº¿ç¨‹æ± æ‰§è¡Œå•æ‰¹æ¨ç†
                batch_texts = await loop.run_in_executor(
                    None, _sync_batch_generate, batch_prompts, batch_temp
                )
                all_generated_texts.extend(batch_texts)

            # è§£ææ‰€æœ‰ç»“æœ
            results = []
            for i, generated_text in enumerate(all_generated_texts):
                workflow_code, is_valid, error = self._parse_workflow_code(
                    generated_text, problem_types[i]
                )
                results.append({
                    "workflow_code": workflow_code,
                    "valid": is_valid,
                    "error": error,
                    "metadata": {
                        "problem": problems[i],
                        "problem_type": problem_types[i],
                        "temperature": temperatures[i]
                    }
                })

            return results

        except Exception as e:
            # å‡ºé”™æ—¶è¿”å›ç©ºç»“æœ
            return [{
                "workflow_code": "",
                "valid": False,
                "error": str(e),
                "metadata": {}
            } for _ in problems]


# ============================================================================
# DSLè§£æå™¨å’Œä»£ç ç”Ÿæˆå™¨ - æç®€ç¬¦å·å¼å·¥ä½œæµ
# ============================================================================

class WorkflowDSLParser:
    """è§£ææç®€DSLç¬¦å·æ ¼å¼

    æ”¯æŒçš„æ ¼å¼:
    - é¡ºåº: "Programmer -> Custom"
    - å¹¶è¡Œ: "[Custom, Custom, Custom] -> ScEnsemble"
    - æ··åˆ: "Programmer -> [Custom, Custom] -> ScEnsemble"
    """

    # æœ‰æ•ˆçš„operatoråˆ—è¡¨
    VALID_OPERATORS = {
        'Custom', 'AnswerGenerate', 'CustomCodeGenerate',
        'Programmer', 'Test', 'Format',
        'Review', 'Revise', 'ScEnsemble', 'MdEnsemble'
    }

    # Operatorè¾“å…¥è¾“å‡ºç±»å‹å®šä¹‰ï¼ˆç”¨äºè‡ªåŠ¨æ¨æ–­å‚æ•°ï¼‰
    OPERATOR_SIGNATURES = {
        'Custom': {
            'inputs': ['input', 'instruction'],
            'output': 'response',
            'output_type': 'str'
        },
        'CustomCodeGenerate': {
            'inputs': ['problem', 'entry_point', 'instruction'],
            'output': 'response',
            'output_type': 'str'
        },
        'Programmer': {
            'inputs': ['problem', 'analysis'],
            'output': 'output',  # ä¹Ÿæœ‰ 'code'
            'output_type': 'str'
        },
        'ScEnsemble': {
            'inputs': ['solutions', 'problem'],
            'output': 'response',
            'output_type': 'str',
            'accepts_list': True  # æ¥å—åˆ—è¡¨è¾“å…¥
        },
        'MdEnsemble': {
            'inputs': ['solutions', 'problem'],
            'output': 'solution',
            'output_type': 'str',
            'accepts_list': True
        },
        'Test': {
            'inputs': ['problem', 'solution', 'entry_point'],
            'output': 'solution',
            'output_type': 'str',
            'has_result': True  # è¿”å› result (bool) å’Œ solution
        },
        'Review': {
            'inputs': ['problem', 'solution'],
            'output': 'feedback',
            'output_type': 'str',
            'has_result': True  # è¿”å› review_result (bool) å’Œ feedback
        },
        'Revise': {
            'inputs': ['problem', 'solution', 'feedback'],
            'output': 'solution',
            'output_type': 'str'
        },
        'Format': {
            'inputs': ['problem', 'solution'],
            'output': 'solution',
            'output_type': 'str'
        },
        'AnswerGenerate': {
            'inputs': ['input'],
            'output': 'answer',  # ä¹Ÿæœ‰ 'thought'
            'output_type': 'str'
        }
    }

    def __init__(self):
        pass

    def parse(self, dsl_text: str) -> dict:
        """è§£æDSLæ–‡æœ¬

        Args:
            dsl_text: DSLæ–‡æœ¬ï¼Œå¦‚ "Programmer -> Custom" æˆ– "[Custom, Custom] -> ScEnsemble"

        Returns:
            {
                'valid': bool,
                'error': str or None,
                'stages': [  # æ‰§è¡Œé˜¶æ®µåˆ—è¡¨
                    {
                        'type': 'single' | 'parallel',
                        'operators': ['Programmer'] | ['Custom', 'Custom', 'Custom'],
                    },
                    ...
                ]
            }
        """
        import re

        # æ¸…ç†è¾“å…¥
        dsl_text = dsl_text.strip()

        # ç§»é™¤å¯èƒ½çš„æ ‡ç­¾
        dsl_text = re.sub(r'</?workflow>', '', dsl_text).strip()

        if not dsl_text:
            return {'valid': False, 'error': 'ç©ºçš„DSL', 'stages': []}

        # ğŸ”§ é¢„å¤„ç†ï¼šå¤„ç†æ¡ä»¶è¯­æ³• "Review ? Revise : done" -> "Review -> Revise"
        # ç®€åŒ–å¤„ç†ï¼šå–æ¡ä»¶ä¸ºçœŸçš„åˆ†æ”¯
        cond_match = re.search(r'(\w+)\s*\?\s*(\w+)\s*:\s*(\w+)', dsl_text)
        if cond_match:
            condition_op, true_branch, false_branch = cond_match.groups()
            # å¦‚æœfalse_branchæ˜¯doneï¼Œå–true_branchï¼›å¦åˆ™éƒ½æ‰§è¡Œ
            if false_branch.lower() == 'done':
                replacement = f"{condition_op} -> {true_branch}"
            else:
                replacement = f"{condition_op} -> {true_branch}"
            dsl_text = re.sub(r'\w+\s*\?\s*\w+\s*:\s*\w+', replacement, dsl_text)

        # ğŸ”§ é¢„å¤„ç†ï¼šç§»é™¤ç»ˆæ­¢ç¬¦ "-> done"
        dsl_text = re.sub(r'->\s*done\s*$', '', dsl_text, flags=re.IGNORECASE).strip()

        stages = []

        # æŒ‰ -> åˆ†å‰²
        parts = [p.strip() for p in dsl_text.split('->')]

        for part in parts:
            if not part:
                continue

            # ğŸ”§ è·³è¿‡doneå…³é”®å­—
            if part.lower() == 'done':
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯å¹¶è¡Œæ ¼å¼ [Op1, Op2, ...]
            if part.startswith('[') and part.endswith(']'):
                # å¹¶è¡Œé˜¶æ®µ
                inner = part[1:-1].strip()
                operators = [op.strip() for op in inner.split(',')]

                # éªŒè¯æ¯ä¸ªoperator
                for op in operators:
                    if op not in self.VALID_OPERATORS:
                        return {'valid': False, 'error': f'æ— æ•ˆçš„operator: {op}', 'stages': []}

                stages.append({
                    'type': 'parallel',
                    'operators': operators
                })
            else:
                # å•ä¸ªoperator
                op = part.strip()
                if op not in self.VALID_OPERATORS:
                    return {'valid': False, 'error': f'æ— æ•ˆçš„operator: {op}', 'stages': []}

                stages.append({
                    'type': 'single',
                    'operators': [op]
                })

        if not stages:
            return {'valid': False, 'error': 'æœªæ‰¾åˆ°æœ‰æ•ˆçš„operator', 'stages': []}

        return {'valid': True, 'error': None, 'stages': stages}


class WorkflowCodeGenerator:
    """å°†è§£æåçš„DSLè½¬æ¢ä¸ºå¯æ‰§è¡Œçš„Python Workflowä»£ç """

    def __init__(self, problem_type: str = 'math'):
        self.problem_type = problem_type
        self.parser = WorkflowDSLParser()

    def generate(self, dsl_text: str) -> Tuple[str, bool, Optional[str]]:
        """ä»DSLç”Ÿæˆå®Œæ•´çš„Workflowä»£ç 

        Args:
            dsl_text: DSLæ–‡æœ¬

        Returns:
            (code, is_valid, error)
        """
        # è§£æDSL
        parsed = self.parser.parse(dsl_text)

        if not parsed['valid']:
            return self._get_default_code(), False, parsed['error']

        stages = parsed['stages']

        # æ”¶é›†æ‰€æœ‰éœ€è¦çš„operators
        all_operators = set()
        for stage in stages:
            all_operators.update(stage['operators'])

        # ç”Ÿæˆä»£ç 
        code = self._generate_workflow_code(stages, all_operators)

        # éªŒè¯è¯­æ³•
        try:
            ast.parse(code)
            return code, True, None
        except SyntaxError as e:
            return self._get_default_code(), False, f"è¯­æ³•é”™è¯¯: {e}"

    def _generate_workflow_code(self, stages: List[dict], all_operators: set) -> str:
        """ç”ŸæˆWorkflowç±»ä»£ç """

        # ç”Ÿæˆ__init__ä¸­çš„operatoråˆå§‹åŒ–
        init_lines = []
        for op in sorted(all_operators):
            attr_name = self._to_snake_case(op)
            init_lines.append(f"        self.{attr_name} = operator.{op}(self.llm)")

        # ç”Ÿæˆ__call__ä¸­çš„æ‰§è¡Œé€»è¾‘
        call_lines = self._generate_call_body(stages)

        # ç»„è£…å®Œæ•´ä»£ç 
        code = f'''class Workflow:
    def __init__(self, name: str, llm_config, dataset):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
{chr(10).join(init_lines)}

    async def __call__(self, problem: str, entry_point: str = None):
        """
        Auto-generated workflow from DSL
        """
{chr(10).join(call_lines)}
'''
        return code

    def _generate_call_body(self, stages: List[dict]) -> List[str]:
        """ç”Ÿæˆ__call__æ–¹æ³•ä½“"""
        lines = []
        prev_output = None  # ä¸Šä¸€é˜¶æ®µçš„è¾“å‡ºå˜é‡å
        prev_is_list = False  # ä¸Šä¸€é˜¶æ®µæ˜¯å¦æ˜¯å¹¶è¡Œï¼ˆè¾“å‡ºåˆ—è¡¨ï¼‰

        for i, stage in enumerate(stages):
            is_last = (i == len(stages) - 1)

            if stage['type'] == 'parallel':
                # å¹¶è¡Œæ‰§è¡Œå¤šä¸ªç›¸åŒoperator
                ops = stage['operators']
                op = ops[0]  # å‡è®¾å¹¶è¡Œæ—¶éƒ½æ˜¯åŒä¸€ç±»å‹
                attr_name = self._to_snake_case(op)
                sig = WorkflowDSLParser.OPERATOR_SIGNATURES.get(op, {})

                # ç”Ÿæˆå¹¶è¡Œè°ƒç”¨
                lines.append(f"        # å¹¶è¡Œæ‰§è¡Œ {len(ops)} ä¸ª {op}")
                lines.append(f"        import asyncio")

                # æ„å»ºå‚æ•°
                if prev_output:
                    input_param = prev_output
                else:
                    input_param = 'problem'

                # ç”Ÿæˆå¹¶è¡Œä»»åŠ¡
                tasks = []
                for j in range(len(ops)):
                    param_str = self._build_params(op, input_param, is_first=(i == 0))
                    tasks.append(f"self.{attr_name}({param_str})")

                lines.append(f"        tasks = [{', '.join(tasks)}]")
                lines.append(f"        results_{i} = await asyncio.gather(*tasks)")
                lines.append(f"        solutions_{i} = [r['{sig.get('output', 'response')}'] for r in results_{i}]")

                prev_output = f"solutions_{i}"
                prev_is_list = True

            else:
                # å•ä¸ªoperator
                op = stage['operators'][0]
                attr_name = self._to_snake_case(op)
                sig = WorkflowDSLParser.OPERATOR_SIGNATURES.get(op, {})

                # æ„å»ºå‚æ•°
                if prev_is_list and sig.get('accepts_list'):
                    # å‰ä¸€é˜¶æ®µæ˜¯åˆ—è¡¨ï¼Œå½“å‰operatoræ¥å—åˆ—è¡¨ï¼ˆå¦‚ScEnsembleï¼‰
                    param_str = f"solutions={prev_output}, problem=problem"
                elif prev_output:
                    param_str = self._build_params(op, prev_output, is_first=False)
                else:
                    param_str = self._build_params(op, 'problem', is_first=True)

                lines.append(f"        result_{i} = await self.{attr_name}({param_str})")

                output_key = sig.get('output', 'response')
                prev_output = f"result_{i}['{output_key}']"
                prev_is_list = False

        # æœ€åè¿”å›
        lines.append(f"        return {prev_output}, self.llm.get_usage_summary()['total_cost']")

        return lines

    def _build_params(self, op: str, input_var: str, is_first: bool) -> str:
        """æ„å»ºoperatorè°ƒç”¨å‚æ•°"""
        sig = WorkflowDSLParser.OPERATOR_SIGNATURES.get(op, {})

        if op == 'Custom':
            return f"input={input_var}, instruction=''"
        elif op == 'CustomCodeGenerate':
            if is_first:
                return f"problem={input_var}, entry_point=entry_point or 'solve', instruction=''"
            else:
                return f"problem=problem, entry_point=entry_point or 'solve', instruction=''"
        elif op == 'Programmer':
            if is_first:
                return f"problem={input_var}, analysis='None'"
            else:
                return f"problem=problem, analysis={input_var}"
        elif op == 'Test':
            return f"problem=problem, solution={input_var}, entry_point=entry_point or 'solve'"
        elif op == 'Review':
            return f"problem=problem, solution={input_var}"
        elif op == 'Revise':
            return f"problem=problem, solution={input_var}, feedback=''"
        elif op == 'Format':
            return f"problem=problem, solution={input_var}"
        elif op == 'AnswerGenerate':
            return f"input={input_var}"
        elif op in ('ScEnsemble', 'MdEnsemble'):
            return f"solutions={input_var}, problem=problem"
        else:
            return f"input={input_var}, instruction=''"

    def _to_snake_case(self, name: str) -> str:
        """é©¼å³°è½¬ä¸‹åˆ’çº¿ï¼šCustomCodeGenerate -> custom_code_generate"""
        import re
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', name)
        return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

    def _get_default_code(self) -> str:
        """é»˜è®¤çš„ç®€å•Workflow"""
        return '''class Workflow:
    def __init__(self, name: str, llm_config, dataset):
        self.name = name
        self.dataset = dataset
        self.llm = create_llm_instance(llm_config)
        self.custom = operator.Custom(self.llm)

    async def __call__(self, problem: str, entry_point: str = None):
        result = await self.custom(input=problem, instruction="")
        return result['response'], self.llm.get_usage_summary()['total_cost']
'''
