#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆå¼‚æ­¥LLMå®¢æˆ·ç«¯ - Plan A + Plan B å®ç°

Plan A: httpxè¿æ¥æ± ä¼˜åŒ–
- è‡ªå®šä¹‰httpxå®¢æˆ·ç«¯é…ç½®
- å¢åŠ max_connectionsåˆ°50
- ä¿æŒé•¿è¿æ¥å¤ç”¨

Plan B: æ‰¹é‡APIæ”¯æŒ
- å¹¶å‘æ‰§è¡Œå¤šä¸ªprompt
- ä½¿ç”¨asyncio.gatherå’Œä¿¡å·é‡æ§åˆ¶å¹¶å‘
"""
import asyncio
from typing import List, Optional, Dict, Any, Tuple
import httpx
from openai import AsyncOpenAI

# å…¨å±€è¿æ¥æ± é…ç½®
_GLOBAL_HTTP_CLIENT: Optional[httpx.AsyncClient] = None
_GLOBAL_SEMAPHORE: Optional[asyncio.Semaphore] = None


def get_optimized_http_client(max_connections: int = 50) -> httpx.AsyncClient:
    """
    è·å–ä¼˜åŒ–çš„httpxå®¢æˆ·ç«¯ï¼ˆå•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åˆ›å»ºï¼‰

    Plan A æ ¸å¿ƒï¼šé…ç½®é«˜å¹¶å‘è¿æ¥æ± 

    Args:
        max_connections: æœ€å¤§è¿æ¥æ•°ï¼ˆé»˜è®¤50ï¼ŒvLLMé€šå¸¸èƒ½å¤„ç†ï¼‰

    Returns:
        é…ç½®å¥½çš„httpx.AsyncClient
    """
    global _GLOBAL_HTTP_CLIENT

    if _GLOBAL_HTTP_CLIENT is None:
        # åˆ›å»ºå¸¦è¿æ¥æ± çš„httpxå®¢æˆ·ç«¯
        limits = httpx.Limits(
            max_connections=max_connections,
            max_keepalive_connections=max_connections,
            keepalive_expiry=30.0  # 30ç§’ä¿æ´»
        )

        timeout = httpx.Timeout(
            connect=10.0,      # è¿æ¥è¶…æ—¶
            read=300.0,        # è¯»å–è¶…æ—¶ï¼ˆLLMç”Ÿæˆå¯èƒ½è¾ƒé•¿ï¼‰
            write=30.0,        # å†™å…¥è¶…æ—¶
            pool=10.0          # ç­‰å¾…è¿æ¥æ± è¶…æ—¶
        )

        _GLOBAL_HTTP_CLIENT = httpx.AsyncClient(
            limits=limits,
            timeout=timeout,
            http2=True,  # å¯ç”¨HTTP/2æå‡å¹¶å‘æ€§èƒ½
        )

        print(f"âœ… åˆ›å»ºä¼˜åŒ–çš„HTTPè¿æ¥æ± : max_connections={max_connections}")

    return _GLOBAL_HTTP_CLIENT


def get_concurrency_semaphore(max_concurrent: int = 20) -> asyncio.Semaphore:
    """
    è·å–å¹¶å‘æ§åˆ¶ä¿¡å·é‡

    Args:
        max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

    Returns:
        asyncio.Semaphore
    """
    global _GLOBAL_SEMAPHORE

    if _GLOBAL_SEMAPHORE is None:
        _GLOBAL_SEMAPHORE = asyncio.Semaphore(max_concurrent)
        print(f"âœ… åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡: max_concurrent={max_concurrent}")

    return _GLOBAL_SEMAPHORE


class OptimizedAsyncLLM:
    """
    ä¼˜åŒ–ç‰ˆå¼‚æ­¥LLMå®¢æˆ·ç«¯

    ç‰¹æ€§:
    - Plan A: httpxè¿æ¥æ± ï¼Œæ”¯æŒé«˜å¹¶å‘HTTPè¯·æ±‚
    - Plan B: æ‰¹é‡APIï¼Œä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªprompt
    - å…¼å®¹åŸAsyncLLMæ¥å£
    """

    def __init__(
        self,
        api_key: str,
        base_url: str,
        model: str,
        temperature: float = 0.7,
        top_p: float = 1.0,
        max_connections: int = 50,
        max_concurrent: int = 20,
        system_msg: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–ç‰ˆLLMå®¢æˆ·ç«¯

        Args:
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            model: æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            top_p: Top-pé‡‡æ ·
            max_connections: æœ€å¤§HTTPè¿æ¥æ•° (Plan A)
            max_concurrent: æœ€å¤§å¹¶å‘è¯·æ±‚æ•° (Plan B)
            system_msg: ç³»ç»Ÿæ¶ˆæ¯
        """
        self.model = model
        self.temperature = temperature
        self.top_p = top_p
        self.system_msg = system_msg

        # Plan A: ä½¿ç”¨ä¼˜åŒ–çš„httpxå®¢æˆ·ç«¯
        http_client = get_optimized_http_client(max_connections)

        # åˆ›å»ºAsyncOpenAIå®¢æˆ·ç«¯ï¼Œæ³¨å…¥è‡ªå®šä¹‰httpxå®¢æˆ·ç«¯
        self.client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            http_client=http_client,
            max_retries=2,
        )

        # Plan B: å¹¶å‘æ§åˆ¶
        self.semaphore = get_concurrency_semaphore(max_concurrent)

        # Tokenç»Ÿè®¡
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.total_calls = 0

    async def __call__(self, prompt: str) -> str:
        """
        å•ä¸ªpromptè°ƒç”¨ï¼ˆå…¼å®¹åŸæ¥å£ï¼‰

        Args:
            prompt: ç”¨æˆ·æç¤ºè¯

        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        async with self.semaphore:  # å¹¶å‘æ§åˆ¶
            messages = []
            if self.system_msg:
                messages.append({"role": "system", "content": self.system_msg})
            messages.append({"role": "user", "content": prompt})

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                top_p=self.top_p,
            )

            # ç»Ÿè®¡token
            if response.usage:
                self.total_input_tokens += response.usage.prompt_tokens
                self.total_output_tokens += response.usage.completion_tokens
            self.total_calls += 1

            return response.choices[0].message.content

    async def aask(self, msg: str, system_msgs: list = None) -> str:
        """å…¼å®¹MetaGPTé£æ ¼çš„aaskæ¥å£"""
        original_sys_msg = self.system_msg
        if system_msgs:
            self.system_msg = system_msgs[0] if isinstance(system_msgs, list) else system_msgs

        try:
            return await self.__call__(msg)
        finally:
            self.system_msg = original_sys_msg

    async def batch_call(
        self,
        prompts: List[str],
        return_exceptions: bool = True
    ) -> List[Tuple[bool, Any]]:
        """
        Plan B: æ‰¹é‡è°ƒç”¨å¤šä¸ªprompt

        ä½¿ç”¨asyncio.gatherå¹¶å‘æ‰§è¡Œï¼Œå¤§å¹…æå‡ååé‡

        Args:
            prompts: å¤šä¸ªpromptåˆ—è¡¨
            return_exceptions: æ˜¯å¦è¿”å›å¼‚å¸¸è€Œä¸æ˜¯æŠ›å‡º

        Returns:
            List of (success: bool, result_or_error: Any)
        """
        async def safe_call(prompt: str) -> Tuple[bool, Any]:
            try:
                result = await self.__call__(prompt)
                return (True, result)
            except Exception as e:
                if return_exceptions:
                    return (False, str(e))
                raise

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰prompts
        results = await asyncio.gather(
            *[safe_call(p) for p in prompts],
            return_exceptions=return_exceptions
        )

        # å¤„ç†gatherè¿”å›çš„å¼‚å¸¸å¯¹è±¡
        processed = []
        for r in results:
            if isinstance(r, Exception):
                processed.append((False, str(r)))
            else:
                processed.append(r)

        return processed

    async def batch_call_with_messages(
        self,
        messages_list: List[List[Dict[str, str]]],
        return_exceptions: bool = True
    ) -> List[Tuple[bool, Any]]:
        """
        Plan Bæ‰©å±•: æ‰¹é‡è°ƒç”¨å®Œæ•´æ¶ˆæ¯æ ¼å¼

        Args:
            messages_list: å¤šä¸ªæ¶ˆæ¯åˆ—è¡¨
            return_exceptions: æ˜¯å¦è¿”å›å¼‚å¸¸

        Returns:
            List of (success, result_or_error)
        """
        async def call_with_messages(messages: List[Dict[str, str]]) -> Tuple[bool, Any]:
            try:
                async with self.semaphore:
                    response = await self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        temperature=self.temperature,
                        top_p=self.top_p,
                    )

                    if response.usage:
                        self.total_input_tokens += response.usage.prompt_tokens
                        self.total_output_tokens += response.usage.completion_tokens
                    self.total_calls += 1

                    return (True, response.choices[0].message.content)
            except Exception as e:
                if return_exceptions:
                    return (False, str(e))
                raise

        results = await asyncio.gather(
            *[call_with_messages(m) for m in messages_list],
            return_exceptions=return_exceptions
        )

        processed = []
        for r in results:
            if isinstance(r, Exception):
                processed.append((False, str(r)))
            else:
                processed.append(r)

        return processed

    def get_usage_summary(self) -> Dict[str, Any]:
        """è·å–ä½¿ç”¨ç»Ÿè®¡"""
        return {
            "total_input_tokens": self.total_input_tokens,
            "total_output_tokens": self.total_output_tokens,
            "total_tokens": self.total_input_tokens + self.total_output_tokens,
            "total_calls": self.total_calls,
            "total_cost": 0.0  # vLLMæœ¬åœ°éƒ¨ç½²æ— æˆæœ¬
        }


def create_optimized_llm_instance(
    config: Dict[str, Any],
    max_connections: int = 50,
    max_concurrent: int = 20
) -> OptimizedAsyncLLM:
    """
    åˆ›å»ºä¼˜åŒ–ç‰ˆLLMå®ä¾‹çš„å·¥å‚å‡½æ•°

    Args:
        config: LLMé…ç½®å­—å…¸ï¼ŒåŒ…å«:
            - api_key: APIå¯†é’¥
            - base_url: APIåŸºç¡€URL
            - model_name: æ¨¡å‹åç§°
            - temperature: ç”Ÿæˆæ¸©åº¦ (å¯é€‰)
            - top_p: Top-pé‡‡æ · (å¯é€‰)
        max_connections: Plan A - æœ€å¤§HTTPè¿æ¥æ•°
        max_concurrent: Plan B - æœ€å¤§å¹¶å‘è¯·æ±‚æ•°

    Returns:
        OptimizedAsyncLLMå®ä¾‹
    """
    return OptimizedAsyncLLM(
        api_key=config.get("api_key", "dummy"),
        base_url=config.get("base_url", "http://localhost:8002/v1"),
        model=config.get("model_name", config.get("model", "gpt-oss-120b")),
        temperature=config.get("temperature", 0.7),
        top_p=config.get("top_p", 1.0),
        max_connections=max_connections,
        max_concurrent=max_concurrent,
    )


async def cleanup_global_resources():
    """æ¸…ç†å…¨å±€èµ„æºï¼ˆç¨‹åºç»“æŸæ—¶è°ƒç”¨ï¼‰"""
    global _GLOBAL_HTTP_CLIENT
    if _GLOBAL_HTTP_CLIENT is not None:
        await _GLOBAL_HTTP_CLIENT.aclose()
        _GLOBAL_HTTP_CLIENT = None
        print("âœ… æ¸…ç†HTTPè¿æ¥æ± å®Œæˆ")


# ============================================================
# æµ‹è¯•ä»£ç 
# ============================================================

async def test_optimized_llm():
    """æµ‹è¯•ä¼˜åŒ–ç‰ˆLLMå®¢æˆ·ç«¯"""
    print("\n" + "=" * 60)
    print("ğŸ§ª æµ‹è¯•ä¼˜åŒ–ç‰ˆLLMå®¢æˆ·ç«¯")
    print("=" * 60)

    # åˆ›å»ºå®ä¾‹
    config = {
        "api_key": "dummy",
        "base_url": "http://localhost:8002/v1",
        "model_name": "/home/yijia/lhy/openai/gpt-oss-120b",
        "temperature": 0.7,
    }

    llm = create_optimized_llm_instance(
        config,
        max_connections=50,
        max_concurrent=20
    )

    # æµ‹è¯•1: å•ä¸ªè°ƒç”¨
    print("\nğŸ“ æµ‹è¯•å•ä¸ªè°ƒç”¨...")
    try:
        result = await llm("What is 2 + 2?")
        print(f"  ç»“æœ: {result[:100]}...")
    except Exception as e:
        print(f"  âŒ é”™è¯¯: {e}")

    # æµ‹è¯•2: æ‰¹é‡è°ƒç”¨
    print("\nğŸ“ æµ‹è¯•æ‰¹é‡è°ƒç”¨ (3ä¸ªprompt)...")
    prompts = [
        "What is 1 + 1?",
        "What is the capital of France?",
        "Write a haiku about coding."
    ]

    import time
    start = time.time()
    try:
        results = await llm.batch_call(prompts)
        elapsed = time.time() - start

        print(f"  è€—æ—¶: {elapsed:.2f}ç§’")
        for i, (success, result) in enumerate(results):
            status = "âœ…" if success else "âŒ"
            preview = str(result)[:50] if result else "N/A"
            print(f"  {status} Prompt {i+1}: {preview}...")
    except Exception as e:
        print(f"  âŒ æ‰¹é‡è°ƒç”¨é”™è¯¯: {e}")

    # ç»Ÿè®¡
    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡:")
    usage = llm.get_usage_summary()
    print(f"  æ€»è°ƒç”¨: {usage['total_calls']}")
    print(f"  æ€»Token: {usage['total_tokens']}")

    # æ¸…ç†
    await cleanup_global_resources()


if __name__ == "__main__":
    asyncio.run(test_optimized_llm())
